{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuGBdialTQzlNuC03oOMFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaibaKhan112/36-Weeks-Preparation-Challenge/blob/main/My_personalized_cv_RAG_%26_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gKPgPC0pshRW",
        "outputId": "3c06e3a7-cb2e-4f01-f55e-4b15aaecb772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy>=2.0,<2.1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "collapsed": true,
        "id": "QDjvboHftln2",
        "outputId": "9ddf21e2-d659-4d8c-b13e-8a88ea87f123"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.1,>=2.0\n",
            "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-pinecone 0.2.13 requires numpy!=2.0.2,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "f28cd8d8c9a14066bc726f805fd5a9bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initailizing Pinecone"
      ],
      "metadata": {
        "id": "4j9vBimluiKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load API Keys**\n",
        "\n",
        "We securely load the Pinecone API key from Google Colab storage so our code can connect to Pinecone without exposing the key publicly."
      ],
      "metadata": {
        "id": "ZWQSXHGOtKIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jBYxuah_uyKp",
        "outputId": "86f4edd9-4133-497d-f60c-2ed8d171e62c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pcsk_3s3bVP_UCKXjsXuYBToomAer1228CvkiundwzzHdWa1TQDFJ3QTwMVLKeYXaBw9jrPNHki'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to Pinecone**\n",
        "\n",
        "We create a connection with Pinecone so our program can create and manage a vector database where embeddings will be stored."
      ],
      "metadata": {
        "id": "Cqo_lTS7tUjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc = Pinecone(PINECONE_API_KEY)\n"
      ],
      "metadata": {
        "id": "_IOp4v75uQd6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Vector Index (if not exists)**\n",
        "\n",
        "If the index does not already exist, we create a new Pinecone index with:\n",
        "\n",
        "â€¢ Correct embedding dimension (3072)\n",
        "\n",
        "â€¢ Cosine similarity for searching\n",
        "\n",
        "â€¢ Cloud serverless storage for scalability\n",
        "\n",
        "This prepares a space to store our document embeddings."
      ],
      "metadata": {
        "id": "nF-xHDgbtfsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"my-langchain1\"\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "  pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=3072,\n",
        "      metric=\"cosine\",\n",
        "      spec={\n",
        "        \"serverless\": {\n",
        "            \"cloud\": \"aws\",\n",
        "            \"region\": \"us-east-1\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    )\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "Xn8-OdP3vVCM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loader"
      ],
      "metadata": {
        "id": "AFm4xZu2Ruw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ“„ Step Names + Simple Explanation\n",
        "\n",
        "**Step 1: Install Required Libraries**\n",
        "\n",
        "Name: Install Document Processing Tools\n",
        "\n",
        "Code:\n",
        "\n",
        "`!pip install -qU langchain-community pypdf`\n",
        "\n",
        "**Explanation:**\n",
        "We install LangChain community tools and the PDF reader library so our program can open and read PDF files.\n",
        "\n",
        "**Step 2: Import PDF Loader**\n",
        "\n",
        "Name: Initialize PDF Document Loader\n",
        "\n",
        "Code:\n",
        "\n",
        "`from langchain_community.document_loaders import PyPDFLoader`\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "We import a ready-made LangChain tool that knows how to extract text from PDF files.\n",
        "\n",
        "**Step 3: Load the PDF File**\n",
        "\n",
        "Name: Load Resume Document\n",
        "\n",
        "Code:\n",
        "\n",
        "`loader = PyPDFLoader(\"Laiba Khan.docx.pdf\")`\n",
        "\n",
        "`document = loader.load()`\n",
        "\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "We open the PDF resume file and convert it into a structured format that our program can work with. Each page becomes a document object containing text and metadata.\n",
        "\n",
        "**Step 4: Verify Loaded Content**\n",
        "\n",
        "Name: Preview Extracted Text\n",
        "\n",
        "Code:\n",
        "\n",
        "`print(document[0])`\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "We print the first page of the document to confirm that the text was successfully extracted from the PDF."
      ],
      "metadata": {
        "id": "IK4V-tmI2igl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community pypdf"
      ],
      "metadata": {
        "id": "2yMvNwsRwSyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2dfdf612-71f4-4e3f-d79a-ec6dcf6e7e32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.7/2.5 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Laiba Khan.docx.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TTPMxF5SABJ",
        "outputId": "c75c1a4e-5430-4aa8-aa65-7a0ca8951591"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = loader.load()"
      ],
      "metadata": {
        "id": "XSMjAjMvSWbJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fCEc9VAWdCwD",
        "outputId": "339cafcb-0c76-418e-bd83-2c9b04400918"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Laiba  Khan  Full  Stack  Developer  |  AI  Enthusiast   itslaibakhan1@gmail.com â€¢  GitHub  â€¢  LinkedIn  â€¢  LeetCode     EDUCATION   ISLAMIA  UNIVERSITY                                     Bahawalpur  BS  Artificial  Intelligence:  CGPA  [3.67  /  4.00]                        Still  doing   HONORS  AND  ACHIEVEMENT   â—  Earned  an  80%  tuition  waiver  during  my  bachelor's  degree  due  to  outstanding  academic  performance.  â—  Awarded  a  merit-based  laptop  by  the  government  of  Pakistan  for  being  in  the  top  students  in  the  University.  â—  Solved  88+  problems  on  LeetCode  to  strengthen  data  structures  and  algorithms  skills.  â—  Secured  a  full  scholarship  for  intermediate  studies  at  Pins  College,  Lodhran,  by  ranking  in  the  top  3%.  â—  Participated  in  3+  international  Hackathons.  â—  Awarded  the  PEEF  Scholarship  by  the  government  of  Pakistan  in  secondary  education.  â—  Led  a  6-week  Data  Structures  &  Algorithms  course  at  iCodeGuru.  â—  Typing  Speed:  40  WPM ,  with  ongoing  improvement  efforts  to  reach  100  WPM .   INTERNATIONAL  HACKATHONS   Reasoning  with  o1                                         (LabLab.ai)  â—  Developed  Clean  It,  an  AI-powered  multi-agent  data  cleaning  system  â—¦  Automates  missing  value  handling,  outlier  correction,  and  data  standardization.  â—  Participated  as  the  project  creator,  contributed  to  feature  development,  and  overall  implementation  of  the  solution.  DOGE  Hackathon                                         (LabLab.ai)  â—  Developed  FormEase,  an  intelligent  form-filling  assistant  â—¦  Integrates  with  a  database  to  auto-fetch  user  data,  highlights  missing  fields,  incorrect  formats,  and  inconsistencies.  â—  Assisted  in  coding  and  feature  development  for  the  project.  Falcon  Hackathon                                         (LabLab.ai)  â—  Developed  MediQuick  AI,  aimed  at  transforming  healthcare  â—¦  Utilizes  innovative  AI  solutions.  â—  Participated  as  a  team  member  and  delivered  a  final  project  presentation  to  the  judging  panel,  showcasing  the  \n",
            "assistantâ€™s\n",
            " \n",
            "features\n",
            " \n",
            "and\n",
            " \n",
            "potential\n",
            " \n",
            "impact.\n",
            "  PROJECTS   \n",
            "AI  Resume  Builder  [Link] \n",
            "â—  Project  Overview:  Developed  an  AI-powered  Resume  Builder  that  generates  professional  resumes  from  \n",
            "user\n",
            " \n",
            "input,\n",
            " \n",
            "focusing\n",
            " \n",
            "on\n",
            " \n",
            "functionality,\n",
            " \n",
            "secure\n",
            " \n",
            "authentication,\n",
            " \n",
            "and\n",
            " \n",
            "AI-driven\n",
            " \n",
            "content\n",
            " \n",
            "suggestions.\n",
            " â—  Key  Features:  Integrated  AI  for  automated  resume  content  generation,  implemented  secure  user  \n",
            "authentication,\n",
            " \n",
            "and\n",
            " \n",
            "provided\n",
            " \n",
            "real-time\n",
            " \n",
            "preview\n",
            " \n",
            "functionality.\n",
            " â—  Technologies  Used:  MERN  Stack  (MongoDB,  Express,  React,  Node.js),  GEMINI  API,  JWT.  â—  Impact:  Simplified  the  resume  creation  process,  helping  users  craft  polished  resumes  efficiently  while  \n",
            "maintaining\n",
            " \n",
            "data\n",
            " \n",
            "security\n",
            " \n",
            "and\n",
            " \n",
            "personalization.' metadata={'producer': 'Skia/PDF m144 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Laiba Khan.docx', 'source': 'Laiba Khan.docx.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Splitter"
      ],
      "metadata": {
        "id": "tRR3zXqIdQC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "shCmNrJ0dFdw",
        "outputId": "11284879-161f-4e03-f19c-28cf0f60655b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=150)\n",
        "chunks = text_splitter.split_documents(document)\n",
        "\n"
      ],
      "metadata": {
        "id": "S8--7RTKdceB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-cGUD3sdrKg",
        "outputId": "dd6fce54-2f0f-45e7-c0f0-739d6fdd01d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XRCSffBfn6Y",
        "outputId": "27650b8c-7e66-4113-9201-a8a1ac7c0e18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Embeddings"
      ],
      "metadata": {
        "id": "zyabFkwpfvkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "GOOGLE_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "W8vnM48RftYg",
        "outputId": "e10d7b7e-cdc1-4700-f1eb-ff56c426c264"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyBqY7RGEQ1uVtsz5wDTVOXcfZnaLCpVzuQ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU  langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ud4MZxnlgKJC",
        "outputId": "a3d3cac5-e8b7-4142-cb52-de70ef7619d5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.4/719.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.5/236.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.48.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\", api_key = GOOGLE_API_KEY)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLB8C-WjgUFP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing vector store"
      ],
      "metadata": {
        "id": "7bwfLXc5jUjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rt96ywpgiUsX",
        "outputId": "62fdf311-412d-41fd-8d30-54a01c3fae72"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/108.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/157.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"numpy==2.0.2\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "collapsed": true,
        "id": "Np7v08fPjvBg",
        "outputId": "be03d971-d10d-4aa6-fb15-71784ccd4666"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0.2\n",
            "  Using cached numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Using cached numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-pinecone 0.2.13 requires numpy!=2.0.2,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c39ca9aef3454727880eac14bdc95d6b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "ddif__wikRiX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding items to vector store"
      ],
      "metadata": {
        "id": "Uvnf5mKqklBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4"
      ],
      "metadata": {
        "id": "_T_aPfLZkj8J"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
        "vector_store.add_documents(documents=chunks, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n8FpVdL0kxM3",
        "outputId": "750277ed-d0c6-41d8-c9ff-2718f1b7a3f8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['857159e2-72a3-46a9-8b75-2c8f0690b6ad',\n",
              " '82bc9ac7-9743-43f5-b177-e3a0cd965599',\n",
              " '6948023b-8622-44e7-89ed-25a9c9bd15f6',\n",
              " '99de0272-6e66-421a-9815-1053563a48f0',\n",
              " 'c0cac937-21e2-48ea-9269-8f5db483fb6d',\n",
              " '04eb2eba-ca5f-498b-a24b-0cf1d07f5354',\n",
              " '62c62846-37a1-4555-9757-f65401b84586',\n",
              " 'c98de956-7e6a-471b-aa20-8305a5bfb2c3',\n",
              " '23c07b70-0d16-4256-9b49-2da9f24dd7ba',\n",
              " '836eebec-5d47-473a-8b4d-cb7bfd3dafff',\n",
              " 'a54142a0-ee2a-4f1e-a563-06f9998bf75e',\n",
              " '3006c618-1106-420e-81c8-fd5d3e253196',\n",
              " '92995f0b-9366-4014-9dee-3b807ce0e689',\n",
              " '574c3c5b-f2c7-4988-95ee-c4637c4d3895',\n",
              " '86c721c6-bdcc-4829-930e-08de28f5d4de',\n",
              " '3c30df1f-0978-4140-af35-7cb91cb99dce',\n",
              " '75ae27a1-96ab-4565-b8d6-e3419a6c1c66',\n",
              " 'bfca4f35-15d5-4596-9b96-e102b100406c',\n",
              " 'f4c96b5f-9e17-46df-a09c-5daddb4d31a3',\n",
              " '422daee6-19a7-4b85-9da4-fba0b138d7f8',\n",
              " 'c7a61f1b-7ed8-4650-bb3f-6d536aa6cb22',\n",
              " '2713aaca-380f-45e6-9136-6659e187c7b7',\n",
              " '011bda1b-8270-400b-9aba-7b6299783f7c',\n",
              " '46e324bf-d28d-4d0b-b24f-cd9fc7f441c1',\n",
              " '24999be1-81ab-4f70-a452-3f99b3996866',\n",
              " 'ca75790c-89b9-46e8-8fbe-b236f1c142de',\n",
              " '2e5e23fd-a566-4922-9164-31f3e39a33d5',\n",
              " '54c9d454-69b0-48f0-98d1-87af0e1de029',\n",
              " 'a023d933-4e89-4c68-b9a9-bb940ebdbd19',\n",
              " '180dc4e6-7026-4508-a360-086e6ad29778',\n",
              " '77760df6-ac9d-4272-a368-48e29bf5b547',\n",
              " '19a81838-31ef-4897-9429-53a0ac9b7a22',\n",
              " '359b713d-9e7c-41ca-990a-6f1eb427d7e7',\n",
              " '7bb8b621-6849-48f9-bd67-45893af74dab',\n",
              " 'c5ce661f-fa7c-47c4-b3bc-6fef3691db33',\n",
              " '59b6c862-8039-441c-b363-60977d3ea202',\n",
              " '632ac488-9b5e-420f-a497-7dcad5f5cb56',\n",
              " '8128e4b0-6568-46d8-9aac-9d0ebea5a617',\n",
              " '69c407fe-2f86-42d0-8cb0-8c7f1011afc7',\n",
              " '2eee5421-c616-4878-9b44-bf4a1e6749c1',\n",
              " 'd4187223-2617-42af-bca6-c560a1b00f58',\n",
              " '09109b17-ae06-452d-9782-5af19a7c6658',\n",
              " '1180d9d3-e81d-4a9c-b6ad-ee12ae698915']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index.describe_index_stats())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoX-FHhPoQGK",
        "outputId": "4fcfdcac-7f6c-49f2-e3e1-75751b29c621"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dimension': 3072,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'': {'vector_count': 43}},\n",
            " 'total_vector_count': 43,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking the Question to PDF ( doing similarity search)"
      ],
      "metadata": {
        "id": "EvxKdPRSo0Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What projects has Laiba Khan built?\"\n",
        "\n",
        "results = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "for r in results:\n",
        "    print(r.page_content)\n",
        "    print(\"----\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w1I0sV9PoYp3",
        "outputId": "9a8b4367-41ea-4432-b139-aefa80a8955c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laiba  Khan  Full  Stack  Developer  |  AI  Enthusiast   itslaibakhan1@gmail.com â€¢  GitHub  â€¢  LinkedIn  â€¢  LeetCode     EDUCATION   ISLAMIA  UNIVERSITY                                     Bahawalpur  BS  Artificial  Intelligence:  CGPA  [3.67  /  4.00]                        Still  doing   HONORS\n",
            "----\n",
            "supporting\n",
            " \n",
            "medical\n",
            " \n",
            "decision-making.\n",
            " \n",
            "UrduVerse  |  Eng  to  Urdu  Translation  App[Link] \n",
            "â—  Project  Overview:  Developed  UrduVerse,  an  application  that  translates  text  from  English  to  Urdu,  \n",
            "providing\n",
            " \n",
            "users\n",
            " \n",
            "with\n",
            " \n",
            "an\n",
            " \n",
            "intuitive\n",
            " \n",
            "and\n",
            " \n",
            "user-friendly\n",
            " \n",
            "interface\n",
            " \n",
            "for\n",
            "----\n",
            "translations,\n",
            " \n",
            "expanding\n",
            " \n",
            "accessibility\n",
            " \n",
            "for\n",
            " \n",
            "non-English\n",
            " \n",
            "speakers. \n",
            "Pakistan  Environment  Analysis  [Link] â—  Project  Overview:  Developed  a  comprehensive  dashboard  for  analyzing  and  visualizing  project\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to LLM"
      ],
      "metadata": {
        "id": "on_jITyQpNTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rkWhf6pTo8Vw",
        "outputId": "7104826c-3d9f-4666-e9cd-f576002f5df7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (4.2.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.60.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.48.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.13.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: cryptography>=38.0.3 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (43.0.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    api_key= GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "O8DT6OoSpW7Y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"What is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbNKSdjJqmYI",
        "outputId": "bcd03d25-02e0-49ea-9ffc-bae27cb2cfec"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I do not have a name. I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bfeaa-aba8-7911-adc5-c823f675a3e4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 6, 'output_tokens': 40, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 22}})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline(from query to database to llm to user)"
      ],
      "metadata": {
        "id": "WpUm5r6sqt-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query):\n",
        "  vector_result = vector_store.similarity_search(query, k=2)\n",
        "\n",
        "  llm_response = llm.invoke(f\"Answer this user query {query}, here are some references to answer {vector_result}\")\n",
        "\n",
        "  return llm_response"
      ],
      "metadata": {
        "id": "DTgyblohqqjX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"Who is Laiba ?\")"
      ],
      "metadata": {
        "id": "jXx8ffMjsC9B"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "teVkYRQvsGyH",
        "outputId": "2346acc5-f08c-4124-99b1-eadae9d73816"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Laiba is **Laiba Khan**, a Full Stack Developer and AI Enthusiast.\\n\\nShe is currently pursuing a BS in Artificial Intelligence at Islamia University in Bahawalpur, with a CGPA of 3.67/4.00.\\n\\nHer work includes developing an application called UrduVerse, which translates text from English to Urdu. She can be contacted via itslaibakhan1@gmail.com and has profiles on GitHub, LinkedIn, and LeetCode.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"What is latest project of laiba created ?\")"
      ],
      "metadata": {
        "id": "q2X7vfq3sPFf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xqbztmmG3vAF",
        "outputId": "990c9825-34c2-42f0-8fc0-231c9f431923"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided documents, Laiba Khan developed **UrduVerse | Eng to Urdu Translation App**.\\n\\nThe documents do not contain information about the creation date of her projects, so it\\'s not possible to determine which one is the \"latest.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"Has laiba taken part in any international hackathon ?\")"
      ],
      "metadata": {
        "id": "r3t7Qyt43xHr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mm_-0Kap38eU",
        "outputId": "e85ca662-d0d3-481b-871f-583e30f5d086"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, Laiba has participated in 3+ international Hackathons.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMieeA2i3-WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working like Chatbot"
      ],
      "metadata": {
        "id": "dYwOrwvu5Lzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query, k=3):\n",
        "    docs = vector_store.similarity_search(query, k=k)\n",
        "    context = \"\\n\\n\".join([f\"- {d.page_content}\" for d in docs])\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "pnkaob-Z5OQj"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []  # stores {\"role\": \"user\"/\"assistant\", \"content\": \"...\"}\n",
        "SYSTEM_STYLE = (\n",
        "    \"You are a helpful chatbot. Answer only using the provided context. \"\n",
        "    \"If the answer is not in the context, say you don't know.\"\n",
        ")\n",
        "\n",
        "def chatbot_reply(query, k=3):\n",
        "    context = retrieve_context(query, k=k)\n",
        "\n",
        "    history_text = \"\\n\".join(\n",
        "        [f\"{m['role'].upper()}: {m['content']}\" for m in chat_history[-8:]]  # keep last 8 turns\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "{SYSTEM_STYLE}\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "CHAT HISTORY:\n",
        "{history_text}\n",
        "\n",
        "USER: {query}\n",
        "ASSISTANT:\n",
        "\"\"\".strip()\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # save to memory\n",
        "    chat_history.append({\"role\": \"user\", \"content\": query})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": str(response)})\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "CUe2u9yC5PcE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    q = input(\"You: \")\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Bot: Bye!\")\n",
        "        break\n",
        "\n",
        "    ans = chatbot_reply(q, k=3)\n",
        "    print(\"Bot:\", ans.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqLP05i5j0z",
        "outputId": "2388a2a6-1f57-4b45-c99c-76e9dc524964"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hi\n",
            "Bot: I don't know.\n",
            "You: what is the name of person\n",
            "Bot: Laiba Khan\n",
            "You: who is laiba\n",
            "Bot: Laiba Khan is a Full Stack Developer and an AI Enthusiast. She is currently pursuing a BS in Artificial Intelligence at Islamia University, Bahawalpur.\n",
            "You: exit\n",
            "Bot: Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDHeiA2X5qB1"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}